# -*- coding: utf-8 -*-
"""CoronaWebScraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ec0V_9HDSDcn5Tk20ztta8HclhC5wzQR
"""

import requests
from bs4 import BeautifulSoup

# Object to hold info for individual states/countries/counties (name, total
# cases, new cases, total deaths, new deaths and active cases)
class State:
  def __init__(self, name, total_cases, new_cases, total_deaths, new_deaths, active_cases):
    self.name = name
    self.total_cases = total_cases if total_cases != "" else '0'
    self.new_cases = new_cases if new_cases != "" else '0'
    self.total_deaths = total_deaths if total_deaths != "" else '0'
    self.new_deaths = new_deaths if new_deaths != "" else '0'
    self.active_cases = active_cases if active_cases != "" else '0'

  def __str__(self):
    return f'{self.name}\nTotal Cases: {self.total_cases}\nNew Cases Today: {self.new_cases}\nTotal Deaths: {self.total_deaths}\nDeaths Today: {self.new_deaths}\nActive Cases: {self.active_cases}'

# Function to scrape COVID-19 data from the Worldometers website for each state
def scrape_state_data():

  data = []
  URL = 'https://www.worldometers.info/coronavirus/country/us/'
  page = requests.get(URL) # Visit Worldometers site and download HTML
  soup = BeautifulSoup(page.content, 'html.parser') # Parse HTML content

  # List of HTML data for each state
  states = soup.find(id='usa_table_countries_today').find('tbody').find_all('tr')
  states.pop(0) # Remove first entry (USA Total)

  # For each state, extract data, create a new State object and add it to data
  for state in states:
    numbers = state.find_all('td')
    new_data = State(numbers[0].text.strip('\n'), numbers[1].text.strip('\n'),\
                     numbers[2].text.strip('\n'), numbers[3].text.strip('\n'),\
                     numbers[4].text.strip('\n'), numbers[5].text.strip('\n'))
    data.append(new_data)

  return data


# Function to scrape COVID-19 data from worldometers website for each Country
def scrape_country_data():

  data = []
  URL = 'https://www.worldometers.info/coronavirus/#countries'
  page = requests.get(URL)
  soup = BeautifulSoup(page.content, 'html.parser') # Parse HTML content

  # List of HTML data for each country
  countries = soup.find(id='main_table_countries_today').find('tbody').find_all('tr')
  # Remove Data from non-countries (ships)
  countries.pop(6)

  # For each country, extract data, create a new State object and add it to data
  for country in countries:
    numbers = country.find_all('td')
    new_data = State(numbers[1].text.strip('\n'), numbers[2].text.strip('\n'),\
                     numbers[3].text.strip('\n'), numbers[4].text.strip('\n'),\
                     numbers[5].text.strip('\n'), numbers[6].text.strip('\n'))
    data.append(new_data)

  return data

info = scrape_state_data()
for data in info:
  print(str(data)+'\n')